{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"saldenisov/recipenlg\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "download_path = kagglehub.dataset_download(\"saldenisov/recipenlg\")\n",
        "\n",
        "# List all files and directories\n",
        "for root, dirs, files in os.walk(download_path):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRPb7i6sjBIH",
        "outputId": "72935b38-10e0-4ef0-cc6b-a08ec1def868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/recipenlg\n",
            "/kaggle/input/recipenlg/dataset/full_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, GPT2LMHeadModel,\n",
        "    TFGPT2LMHeadModel,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    Trainer, TrainingArguments,\n",
        "    TextDataset, DataCollatorForLanguageModeling,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set dataset path\n",
        "#DATASET_PATH = \"/root/.cache/kagglehub/datasets/saldenisov/recipenlg/versions/1/dataset/\"\n",
        "DATASET_PATH = \"/kaggle/input/recipenlg/dataset/\"\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(os.path.join(path, \"full_dataset.csv\"))\n",
        "\n",
        "    # Select and rename the required columns\n",
        "    df = df[['title', 'NER', 'directions']].dropna()\n",
        "    df = df.rename(columns={\n",
        "        'NER': 'ingredients',\n",
        "        'directions': 'instructions'\n",
        "    })\n",
        "\n",
        "    # Construct the prompt for model input\n",
        "    df['prompt'] = \"Title: \" + df['title'] + \"; Ingredients: \" + df['ingredients']\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    return train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "train_df, val_df = load_data(DATASET_PATH)"
      ],
      "metadata": {
        "id": "Hj9ihxAnx2F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== GPT-2 (PyTorch) ==========\n",
        "\n",
        "def prepare_gpt2_data(train_texts, val_texts, tokenizer, max_length=512):\n",
        "    def save_to_file(data, filename):\n",
        "        with open(filename, \"w\") as f:\n",
        "            for line in data:\n",
        "                f.write(line + \"\\n\")\n",
        "\n",
        "    train_file = \"gpt2_train.txt\"\n",
        "    val_file = \"gpt2_val.txt\"\n",
        "    save_to_file(train_texts, train_file)\n",
        "    save_to_file(val_texts, val_file)\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=max_length\n",
        "    )\n",
        "    val_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=val_file,\n",
        "        block_size=max_length\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def train_gpt2_pytorch():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    train_texts = (train_df[\"prompt\"] + \"\\n\" + train_df[\"instructions\"]).tolist()\n",
        "    val_texts = (val_df[\"prompt\"] + \"\\n\" + val_df[\"instructions\"]).tolist()\n",
        "\n",
        "    train_dataset, val_dataset = prepare_gpt2_data(train_texts, val_texts, tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-pytorch-checkpoints\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "3TDpLxQEyDfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== GPT-2 (TensorFlow) ==========\n",
        "\n",
        "def train_gpt2_tensorflow():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        (train_df[\"prompt\"] + \"\\n\" + train_df[\"instructions\"]).tolist(),\n",
        "        return_tensors=\"tf\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n",
        "    model.fit(inputs[\"input_ids\"], inputs[\"input_ids\"], epochs=3, batch_size=4)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "bqZo9lu_yBVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== BART (PyTorch) ==========\n",
        "\n",
        "def train_bart():\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        train_df[\"prompt\"].tolist(),\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    targets = tokenizer(\n",
        "        train_df[\"instructions\"].tolist(),\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    class RecipeDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, targets):\n",
        "            self.encodings = encodings\n",
        "            self.targets = targets\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.encodings.input_ids)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return {\n",
        "                \"input_ids\": self.encodings.input_ids[idx],\n",
        "                \"attention_mask\": self.encodings.attention_mask[idx],\n",
        "                \"labels\": self.targets.input_ids[idx]\n",
        "            }\n",
        "\n",
        "    dataset = RecipeDataset(inputs, targets)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./bart-checkpoints\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        logging_steps=10,\n",
        "        save_steps=1000,\n",
        "        evaluation_strategy=\"no\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "4D8ZyvgFx8B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which model to run\n",
        "if __name__ == \"__main__\":\n",
        "    model_type = \"bart\"  # Change to \"gpt2_tf\" or \"gpt2_pt\"\n",
        "\n",
        "    if model_type == \"gpt2_pt\":\n",
        "        model, tokenizer = train_gpt2_pytorch()\n",
        "    elif model_type == \"gpt2_tf\":\n",
        "        model, tokenizer = train_gpt2_tensorflow()\n",
        "    elif model_type == \"bart\":\n",
        "        model, tokenizer = train_bart()"
      ],
      "metadata": {
        "id": "WQLUfQU3x6NL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}