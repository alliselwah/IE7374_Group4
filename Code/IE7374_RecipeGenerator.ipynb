{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"saldenisov/recipenlg\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "download_path = kagglehub.dataset_download(\"saldenisov/recipenlg\")\n",
        "\n",
        "# List all files and directories\n",
        "for root, dirs, files in os.walk(download_path):\n",
        "    for file in files:\n",
        "        print(os.path.join(root, file))\n"
      ],
      "metadata": {
        "id": "GRPb7i6sjBIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88a8d62-6fbf-4d0e-b5e0-d4bdace96775"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/saldenisov/recipenlg?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 638M/638M [00:16<00:00, 40.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/saldenisov/recipenlg/versions/1\n",
            "/root/.cache/kagglehub/datasets/saldenisov/recipenlg/versions/1/dataset/full_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    GPT2Tokenizer, GPT2LMHeadModel,\n",
        "    TFGPT2LMHeadModel,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    Trainer, TrainingArguments,\n",
        "    TextDataset, DataCollatorForLanguageModeling,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Set dataset path\n",
        "DATASET_PATH = \"/kaggle/input/recipenlg/dataset/\"\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "def load_data(path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(os.path.join(path, \"full_dataset.csv\"))\n",
        "\n",
        "    # Select and rename the required columns\n",
        "    df = df[['title', 'NER', 'directions']].dropna()\n",
        "    df = df.rename(columns={\n",
        "        'NER': 'ingredients',\n",
        "        'directions': 'instructions'\n",
        "    })\n",
        "\n",
        "    # Construct the prompt for model input\n",
        "    df['prompt'] = \"Title: \" + df['title'] + \"; Ingredients: \" + df['ingredients']\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    return train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "train_df, val_df = load_data(DATASET_PATH)"
      ],
      "metadata": {
        "id": "Hj9ihxAnx2F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "ee226ec6-bcdd-42c4-b173-8adae381c667"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/recipenlg/dataset/full_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-4004981990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2-4004981990.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Read the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Select and rename the required columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/recipenlg/dataset/full_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== GPT-2 (PyTorch) ==========\n",
        "\n",
        "def prepare_gpt2_data(train_texts, val_texts, tokenizer, max_length=512):\n",
        "    def save_to_file(data, filename):\n",
        "        with open(filename, \"w\") as f:\n",
        "            for line in data:\n",
        "                f.write(line + \"\\n\")\n",
        "\n",
        "    train_file = \"gpt2_train.txt\"\n",
        "    val_file = \"gpt2_val.txt\"\n",
        "    save_to_file(train_texts, train_file)\n",
        "    save_to_file(val_texts, val_file)\n",
        "\n",
        "    train_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=train_file,\n",
        "        block_size=max_length\n",
        "    )\n",
        "    val_dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=val_file,\n",
        "        block_size=max_length\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def train_gpt2_pytorch():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    train_texts = (train_df[\"prompt\"] + \"\\n\" + train_df[\"instructions\"]).tolist()\n",
        "    val_texts = (val_df[\"prompt\"] + \"\\n\" + val_df[\"instructions\"]).tolist()\n",
        "\n",
        "    train_dataset, val_dataset = prepare_gpt2_data(train_texts, val_texts, tokenizer)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2-pytorch-checkpoints\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        save_steps=1000,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "3TDpLxQEyDfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== GPT-2 (TensorFlow) ==========\n",
        "\n",
        "def train_gpt2_tensorflow():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        (train_df[\"prompt\"] + \"\\n\" + train_df[\"instructions\"]).tolist(),\n",
        "        return_tensors=\"tf\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5))\n",
        "    model.fit(inputs[\"input_ids\"], inputs[\"input_ids\"], epochs=3, batch_size=4)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "bqZo9lu_yBVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== BART (PyTorch) ==========\n",
        "\n",
        "def train_bart():\n",
        "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        train_df[\"prompt\"].tolist(),\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    targets = tokenizer(\n",
        "        train_df[\"instructions\"].tolist(),\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    class RecipeDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, targets):\n",
        "            self.encodings = encodings\n",
        "            self.targets = targets\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.encodings.input_ids)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return {\n",
        "                \"input_ids\": self.encodings.input_ids[idx],\n",
        "                \"attention_mask\": self.encodings.attention_mask[idx],\n",
        "                \"labels\": self.targets.input_ids[idx]\n",
        "            }\n",
        "\n",
        "    dataset = RecipeDataset(inputs, targets)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./bart-checkpoints\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        logging_steps=10,\n",
        "        save_steps=1000,\n",
        "        evaluation_strategy=\"no\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "4D8ZyvgFx8B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IN TESTING\n",
        "\n",
        "def evaluate_bart_bleu(model, tokenizer, val_df, num_samples=10):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        input_text = val_df[\"prompt\"].iloc[i]\n",
        "        reference = val_df[\"instructions\"].iloc[i]\n",
        "\n",
        "        # Encode the input and generate prediction\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        # Decode the output\n",
        "        predicted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # BLEU score\n",
        "        reference_tokens = [reference.lower().split()]\n",
        "        candidate_tokens = predicted.lower().split()\n",
        "        score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "        print(f\"\\nExample {i+1}\")\n",
        "        print(\"Prompt:\", input_text)\n",
        "        print(\"Reference:\", reference)\n",
        "        print(\"Generated:\", predicted)\n",
        "        print(f\"BLEU Score: {round(score, 4)}\")\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"\\nAverage BLEU Score over {num_samples} samples: {round(avg_bleu, 4)}\")\n",
        "\n",
        "model, tokenizer = train_bart()\n",
        "evaluate_bart_bleu(model, tokenizer, val_df, num_samples=10)"
      ],
      "metadata": {
        "id": "VE7qJLoI_D0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IN TESTING\n",
        "\n",
        "def evaluate_gpt2_bleu(model, tokenizer, val_df, num_samples=10):\n",
        "    model.eval()\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        prompt = val_df[\"prompt\"].iloc[i]\n",
        "        reference = val_df[\"instructions\"].iloc[i]\n",
        "\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        predicted = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        reference_tokens = [reference.lower().split()]\n",
        "        candidate_tokens = predicted.lower().split()\n",
        "        score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "        print(f\"\\nExample {i+1}\")\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Reference:\", reference)\n",
        "        print(\"Generated:\", predicted)\n",
        "        print(f\"BLEU Score: {round(score, 4)}\")\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"\\nAverage BLEU Score over {num_samples} samples: {round(avg_bleu, 4)}\")\n",
        "\n",
        "model, tokenizer = train_gpt2_pytorch()\n",
        "evaluate_gpt2_bleu(model, tokenizer, val_df, num_samples=10)"
      ],
      "metadata": {
        "id": "sIzpmQ3y_ztk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_gpt2_tensorflow_bleu(model, tokenizer, val_df, num_samples=10):\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        prompt = val_df[\"prompt\"].iloc[i]\n",
        "        reference = val_df[\"instructions\"].iloc[i]\n",
        "\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"tf\", truncation=True, max_length=512)\n",
        "\n",
        "        # Generate text from model\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the prediction\n",
        "        predicted = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Tokenize and compute BLEU\n",
        "        reference_tokens = [reference.lower().split()]\n",
        "        candidate_tokens = predicted.lower().split()\n",
        "        score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "        print(f\"\\nExample {i+1}\")\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Reference:\", reference)\n",
        "        print(\"Generated:\", predicted)\n",
        "        print(f\"BLEU Score: {round(score, 4)}\")\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"\\nAverage BLEU Score over {num_samples} samples: {round(avg_bleu, 4)}\")\n",
        "\n",
        "model, tokenizer = train_gpt2_tensorflow()\n",
        "evaluate_gpt2_tensorflow_bleu(model, tokenizer, val_df, num_samples=10)"
      ],
      "metadata": {
        "id": "ufbTED5cAhfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which model to run\n",
        "if __name__ == \"__main__\":\n",
        "    model_type = \"gpt2_pt\"  # Change to \"gpt2_tf\" or \"bart\"\n",
        "\n",
        "    if model_type == \"gpt2_pt\":\n",
        "        model, tokenizer = train_gpt2_pytorch()\n",
        "        evaluate_gpt2_bleu(model, tokenizer, val_df)\n",
        "    elif model_type == \"gpt2_tf\":\n",
        "        model, tokenizer = train_gpt2_tensorflow()\n",
        "        evaluate_gpt2_tensorflow_bleu(model, tokenizer, val_df)\n",
        "    elif model_type == \"bart\":\n",
        "        model, tokenizer = train_bart()\n",
        "        evaluate_bart_bleu(model, tokenizer, val_df)"
      ],
      "metadata": {
        "id": "WQLUfQU3x6NL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}